{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Learning algorithm\n",
    "\n",
    "This solution is almost an exact copy of my [previous project](https://github.com/AlekseySyryh/DRL_ContinuousControl). However, there are a few differences.\n",
    "\n",
    "1. I have two identical but independent Deep Deterministic Policy Gradients (DDPG) agents. Of course, I could wrap them in one agent, but it seems to me that the existing implementation emphasizes their independence.\n",
    "\n",
    "1. Normal noise does not work here. So I am try using Ornsteinâ€“Uhlenbeck process instead and it works very well.\n",
    "\n",
    "1. For balance exploration-exploitation tradeoff, I am using epsilon parameter which decreasing from 1 to 0.1 (but probably it will be solved faster) - and it is the likelihood that noise will be used in this episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Plot of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('scores.pkl','rb') as f:\n",
    "    stat=pkl.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data=pd.DataFrame({\"Score\":stat})\n",
    "data[\"Mean\"]=data.rolling(100).mean()\n",
    "data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment solved in 1946 episodes\n",
    "\n",
    "## III. Solution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='c:/Tennis_Windows_x86_64/Tennis.exe');\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import Actor\n",
    "actor1 = Actor(24,2,1)\n",
    "actor2 = Actor(24,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "actor1.load_state_dict(torch.load('actor1.final.pth'))\n",
    "actor2.load_state_dict(torch.load('actor2.final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.600000038743019\n",
      "2.7000000402331352\n",
      "2.600000038743019\n",
      "2.7000000402331352\n",
      "2.7000000402331352\n"
     ]
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    state = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    score = np.zeros(2)\n",
    "    while True:\n",
    "        action=[actor1(torch.tensor(state[0],dtype=torch.float32)).detach().numpy(),actor2(torch.tensor(state[1],dtype=torch.float32)).detach().numpy()]\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        score += env_info.rewards                         # update the score (for each agent)        \n",
    "        state = next_state # roll over states to next time step\n",
    "        if np.any(dones):\n",
    "            print(score.max())\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting effect. In training, in 50% of cases, the agent managed to hit the ball no more than two times. I thought it was some kind of learning problem, but here (without noise) there is no such problem. So the algorithm looks good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Ideas for future work\n",
    "\n",
    "Collaboration looks good, and agents can play long enough now. Now it's time to start Competition part.\n",
    "\n",
    "What if reward of one agent will be also a penalty of another (may be with some discount)? I do not think that such an approach would have worked when learning from scratch, but for an agent who was trained on this task, this might work (some kind of transfer learning).\n",
    "\n",
    "As usual, I will probably check out some other algorithms, but there is not much point in this - the result is already as close as possible to the ideal one. I never managed to see how the agent is mistaken."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
